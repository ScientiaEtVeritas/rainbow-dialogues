{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please backup existing pt files: ../data/cornell_raw.train*.pt, to avoid overwriting them!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"../data/cornell movie-dialogs corpus/src_movie_lines.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines.txt\" --save_data ../data/cornell_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please backup existing pt files: ../data/cornell_raw_min_30_10.train*.pt, to avoid overwriting them!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"../data/cornell movie-dialogs corpus/src_movie_lines.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines.txt\" --save_data ../data/cornell_raw_min_30_10 --src_words_min_frequency 30 --tgt_words_min_frequency 30 --src_seq_length 10 --tgt_seq_length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fields = torch.load(\"../data/cornell_raw_min_30_10.vocab.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token] #stoi: mapping token strings to numerical identifiers.\n",
    "# ['<unk>', '<blank>', 'I', 'you', 'the', 'to', 'a', 'of', 'and', 'You']\n",
    "# src_text_field.pad_token : '<blank>'\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = len(src_vocab)\n",
    "config.tgt_vocab_size = len(tgt_vocab)\n",
    "config.src_padding = src_padding\n",
    "config.tgt_padding = tgt_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.tgt_unk = tgt_vocab.stoi[tgt_text_field.unk_token]\n",
    "config.tgt_bos = tgt_vocab.stoi[tgt_text_field.init_token]\n",
    "config.tgt_eos = tgt_vocab.stoi[tgt_text_field.eos_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq-DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/NoisyLinear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/DQN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config, DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (encoder_embeddings): Embeddings(\n",
       "    (make_embedding): Sequential(\n",
       "      (emb_luts): Elementwise(\n",
       "        (0): Embedding(1462, 100, padding_idx=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): RNNEncoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (make_embedding): Sequential(\n",
       "        (emb_luts): Elementwise(\n",
       "          (0): Embedding(1462, 100, padding_idx=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rnn): LSTM(100, 250, bidirectional=True)\n",
       "  )\n",
       "  (decoder_embeddings): Embeddings(\n",
       "    (make_embedding): Sequential(\n",
       "      (emb_luts): Elementwise(\n",
       "        (0): Embedding(1472, 100, padding_idx=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): InputFeedRNNDecoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (make_embedding): Sequential(\n",
       "        (emb_luts): Elementwise(\n",
       "          (0): Embedding(1472, 100, padding_idx=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0)\n",
       "    (rnn): StackedLSTM(\n",
       "      (dropout): Dropout(p=0.0)\n",
       "      (layers): ModuleList(\n",
       "        (0): LSTMCell(600, 500)\n",
       "      )\n",
       "    )\n",
       "    (attn): GlobalAttention(\n",
       "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
       "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): NoisyLinear()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.current_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/MSELoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MSELoss(\n",
    "    nn.MSELoss(reduction=\"none\"),\n",
    "    model.current_model.generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def __call__(self, src, output):\n",
    "        return 1\n",
    "\n",
    "reward = Reward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1\n",
    "torch_optimizer = torch.optim.SGD(model.current_model.parameters(), lr=lr)\n",
    "optim = onmt.utils.optimizers.Optimizer(torch_optimizer, learning_rate=lr, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_manager = onmt.utils.ReportMgr(report_every=1, start_time=None, tensorboard_writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/QLearning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QLearning(config,\n",
    "                    model,\n",
    "                    reward=reward,\n",
    "                    train_loss=loss,\n",
    "                    valid_loss=loss,\n",
    "                    optim=optim,\n",
    "                    gpu_verbose_level=100)\n",
    "                    #shard_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "train_data_file = \"../data/cornell_raw_min_30_10.train.0.pt\"\n",
    "train_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[train_data_file],\n",
    "                                                     fields=vocab_fields,\n",
    "                                                     batch_size=1,\n",
    "                                                     batch_size_multiple=1,\n",
    "                                                     batch_size_fn=None,\n",
    "                                                     device=\"cpu\",\n",
    "                                                     is_train=True,\n",
    "                                                     repeat=False,\n",
    "                                                     pool_factor=8192)\n",
    "\n",
    "# Preload Experience Replay Buffer\n",
    "\n",
    "if len(model.replay_memory) == 0:\n",
    "    data = list(train_iter)\n",
    "    for example in data[:100]:\n",
    "        model.replay_memory.push(example.src[0].squeeze(1), example.tgt.squeeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in model.target_model.parameters():\n",
    "#    print(i.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in model.replay_memory._storage:\n",
    "    #print((t[1] == 0).sum())\n",
    "    #print(t[1].size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training loop and validate every 200 steps...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA 0.40006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch Length: tensor([9, 9, 8, 8, 8, 8, 7, 7, 6, 6, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3,\n",
      "        3, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1287,  679,  546,    2,    2,    2,    2,    2,    2, 1287])]\n",
      "[tensor([ 103,  961,  551,  911, 1006,  546,    2,    2,    2,    2])]\n",
      "[tensor([1147, 1147, 1147,  968, 1057,  390, 1057, 1057,  390,  925])]\n",
      "[tensor([1147, 1147, 1147,  968, 1057,  390, 1057, 1057,  390,  925])]\n",
      "[tensor([1147, 1147, 1147,  968, 1057,  390, 1057, 1057,  390,  925])]\n",
      "[tensor([ 751,  751,  227,  486, 1122,  671,  393,  760,  982,  313])]\n",
      "[tensor([ 961,  864, 1056,  546,  546,    2,    2,    2,    2,    2])]\n",
      "[tensor([184, 763, 770, 763, 551, 180, 887, 372, 551, 180])]\n",
      "[tensor([ 196,  450,  897, 1437, 1437,  854,  854,  854,  450,  860])]\n",
      "[tensor([ 196,  450,  897, 1437, 1437,  854,  854,  854,  450,  860])]\n",
      "[tensor([1171, 1294, 1294,   60, 1113, 1113, 1297,  551, 1265,   60])]\n",
      "[tensor([1401,  760,  136,  227,  390,  925, 1401,  760,  136,   96])]\n",
      "[tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]\n",
      "[tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]\n",
      "[tensor([ 961,  313,  313,  313,  313,  982,  313, 1201,  631,  313])]\n",
      "[tensor([1171, 1294, 1294,   60, 1113, 1113, 1297,  551, 1265,   60])]\n",
      "[tensor([1401,  760,  136,  227,  390,  925, 1401,  760,  136,   96])]\n",
      "[tensor([1401, 1076, 1184,  469,  953,   33,  195, 1006,  969,  969])]\n",
      "[tensor([ 184,  530,  530, 1249, 1006,  530,  996, 1327, 1327, 1327])]\n",
      "[tensor([313, 313, 313, 313, 313, 313, 313, 313, 313, 313])]\n",
      "[tensor([ 196, 1199,  760, 1136, 1287,  760,  982,  497, 1136,  760])]\n",
      "[tensor([ 103, 1422,  860,  551, 1247,  456, 1247, 1375, 1342,  860])]\n",
      "[tensor([ 103, 1422,  860,  551, 1247,  456, 1247, 1375, 1342,  860])]\n",
      "[tensor([ 897, 1400,  509,  802,  766,  285,  707,  279,  213,  601])]\n",
      "[tensor([ 897,  897,  897, 1136,  897,  766,  102,  291,  766,  475])]\n",
      "[tensor([751, 751, 751, 751, 154, 693,  49,  49,  49,  49])]\n",
      "[tensor([103, 961, 760, 982, 760, 982, 760, 982, 760, 982])]\n",
      "[tensor([1436, 1422,  551,  497,  497,   48, 1307,  458,  313, 1057])]\n",
      "[tensor([751, 751, 751, 751, 154, 693,  49,  49,  49,  49])]\n",
      "[tensor([ 196,  925, 1287,  546,    2,    2,    2,    2,    2,    2])]\n",
      "[tensor([1184, 1401, 1076,  705,  705,  531,  531, 1401, 1076,  705])]\n",
      "[tensor([1184,  789,  968, 1169,  766, 1111, 1342, 1342,  948, 1111])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:tensor(5.8180, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA 0.40012000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch Length: tensor([9, 7, 7, 7, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 1, 1], dtype=torch.int16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n",
      "[tensor([0, 0, 3])]\n",
      "[tensor([0, 0, 0, 3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:tensor(3.8723, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA 0.40018000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch Length: tensor([10, 10,  9,  8,  7,  7,  7,  7,  7,  6,  6,  6,  5,  5,  5,  5,  5,  5,\n",
      "         4,  4,  4,  4,  3,  3,  2,  2,  2,  2,  2,  2,  1,  1],\n",
      "       dtype=torch.int16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 0: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([56,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 2: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([72,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 4: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 5: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([72,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 7: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([72,  3])]\n",
      "[tensor([56,  3])]\n",
      "[tensor([72,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 11: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 12: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 13: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 14: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([56, 56,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 16: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([56, 56,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 18: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 19: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 20: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 21: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 22: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 23: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 24: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 25: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 26: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 27: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 28: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([56, 56,  3])]\n",
      "[tensor([56,  3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 31: No output sentence generated (just </s>)\n",
      "INFO:root:tensor(2.3161, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA 0.40024000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch Length: tensor([8, 7, 7, 7, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 3, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 1, 1, 1, 1], dtype=torch.int16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:tensor(2.5808, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA 0.40030000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch Length: tensor([9, 9, 8, 8, 7, 7, 7, 7, 7, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 3, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1], dtype=torch.int16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 0: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 1: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 2: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 3: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 4: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 5: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 6: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 7: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 8: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 9: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 10: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 11: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 12: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 13: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 3])]\n",
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 15: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 16: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 17: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 18: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 19: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 20: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 21: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 22: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 23: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 24: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 25: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 26: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 27: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 28: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 29: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 30: No output sentence generated (just </s>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Inference 31: No output sentence generated (just </s>)\n",
      "INFO:root:tensor(3.6689, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a99fde02468e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\rl-adventure\\rainbow_dialogues\\modules\\QLearning.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_steps, save_checkpoint_steps, valid_steps)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m#    logger.info(\"GpuRank %d: reduce_counter: %d\" % (self.gpu_rank, i + 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_update_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\rl-adventure\\rainbow_dialogues\\modules\\QLearning.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, batch, normalization)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# maybe already backwarded in train_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabian\\.virtualenvs\\rl-adventure-xhdax2fk\\lib\\site-packages\\onmt\\utils\\optimizers.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabian\\.virtualenvs\\rl-adventure-xhdax2fk\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabian\\.virtualenvs\\rl-adventure-xhdax2fk\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = trainer.train(train_steps=400, valid_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

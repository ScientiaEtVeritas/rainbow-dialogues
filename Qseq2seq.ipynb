{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import numpy.lib.recfunctions as nlr\n",
    "def show(x): print(nlr.unstructured_to_structured(x.numpy())) # pretty print for tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Config.py\n",
    "config.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!perl ../OpenNMT-py/tools/tokenizer.perl  -l en \\\n",
    "< ../data/\"cornell movie-dialogs corpus\"/src_movie_lines.txt \\\n",
    "> ../data/\"cornell movie-dialogs corpus\"/src_movie_lines_tok.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenizer Version 1.1', 'Language: en', 'Number of threads: 1']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!perl ../OpenNMT-py/tools/tokenizer.perl  -l en \\\n",
    "< ../data/\"cornell movie-dialogs corpus\"/tgt_movie_lines.txt \\\n",
    "> ../data/\"cornell movie-dialogs corpus\"/tgt_movie_lines_tok.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw_min_30_10_tok --src_words_min_frequency 30 --tgt_words_min_frequency 30 --src_seq_length 10 --tgt_seq_length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw_min_100_tok --src_words_min_frequency 100 --tgt_words_min_frequency 100 --src_seq_length 10 --tgt_seq_length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2019-10-11 03:37:49,444 INFO] Extracting features...',\n",
       " '[2019-10-11 03:37:49,444 INFO]  * number of source features: 0.',\n",
       " '[2019-10-11 03:37:49,444 INFO]  * number of target features: 0.',\n",
       " '[2019-10-11 03:37:49,444 INFO] Building `Fields` object...',\n",
       " '[2019-10-11 03:37:49,444 INFO] Building & saving training data...',\n",
       " '[2019-10-11 03:37:49,444 INFO] Reading source and target files: data/cornell movie-dialogs corpus/src_movie_lines_tok.txt ../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt.',\n",
       " '[2019-10-11 03:37:49,506 INFO] Building shard 0.',\n",
       " '[2019-10-11 03:37:54,083 INFO]  * saving 0th train data shard to ../data/cornell_raw_min_680_tok.train.0.pt.',\n",
       " '[2019-10-11 03:37:55,793 INFO]  * tgt vocab size: 108.',\n",
       " '[2019-10-11 03:37:55,824 INFO]  * src vocab size: 111.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw_min_680_tok --src_words_min_frequency 680 --tgt_words_min_frequency 680 --src_seq_length 12 --tgt_seq_length 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fields = torch.load(config.dataset + \".vocab.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token] #stoi: mapping token strings to numerical identifiers.\n",
    "# ['<unk>', '<blank>', 'I', 'you', 'the', 'to', 'a', 'of', 'and', 'You']\n",
    "# src_text_field.pad_token : '<blank>'\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = len(src_vocab)\n",
    "config.tgt_vocab_size = len(tgt_vocab)\n",
    "config.src_padding = src_padding\n",
    "config.tgt_padding = tgt_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_unk = src_vocab.stoi[src_text_field.unk_token]\n",
    "config.tgt_unk = tgt_vocab.stoi[tgt_text_field.unk_token]\n",
    "config.tgt_bos = tgt_vocab.stoi[tgt_text_field.init_token]\n",
    "config.tgt_eos = tgt_vocab.stoi[tgt_text_field.eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.vocab_fields = vocab_fields\n",
    "config.src_vocab = src_vocab\n",
    "config.tgt_vocab = tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading dataset from data/cornell_raw_min_680_tok.train.0.pt\n",
      "INFO:root:number of examples: 90893\n"
     ]
    }
   ],
   "source": [
    "import onmt\n",
    "from itertools import chain\n",
    "\n",
    "train_data_file = config.dataset + \".train.0.pt\"\n",
    "train_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[train_data_file],\n",
    "                                                     fields=vocab_fields,\n",
    "                                                     batch_size=1,\n",
    "                                                     batch_size_multiple=1,\n",
    "                                                     batch_size_fn=None,\n",
    "                                                     device=config.device,\n",
    "                                                     is_train=True,\n",
    "                                                     repeat=False,\n",
    "                                                     pool_factor=8192)\n",
    "\n",
    "data = list(train_iter)\n",
    "filtered_data = []\n",
    "for x in data:\n",
    "    # Filtering sentences with <unk> token\n",
    "    if not ((x.src[0].squeeze() == config.src_unk).any() or (x.tgt.squeeze() == config.tgt_unk).any()):\n",
    "        filtered_data.append(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.PRELOADING_SIZE = len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### 1,311 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### 111 src vocabulary size"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### 108 tgt vocabulary size"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Vocabulary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### [&lt;blank&gt;] [&lt;s&gt;] [&lt;/s&gt;] [.] [?] [I] [,] [you] [&apos;s] [...] [!] [the] [&apos;t] [to] [a] [it] [You] [What] [me] [that] [No] [&apos;m] [is] [do] [know] [It] [&apos;re] [of] [don] [in] [--] [Yes] [what] [have] [not] [your] [for] [Yeah] [was] [my] [are] [be] [on] [&apos;ll] [That] [this] [Oh] [can] [He] [The] [about] [he] [We] [we] [here] [and] [him] [How] [like] [with] [want] [think] [Why] [just] [&apos;] [get] [right] [Well] [got] [up] [And] [out] [go] [did] [all] [there] [her] [&apos;ve] [Where] [going] [see] [one] [But] [so] [at] [A] [Who] [no] [She] [They] [say] [they] [Not] [Don] [mean] [&apos;d] [she] [Just] [didn] [Do] [&quot;] [now] [good] [So] [too] [back] [time]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### seq2seq - Hyperparameter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Embedding Size: 100"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### RNN Size: 500"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Pretrain Model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### RL - Hyperparameter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Update Target Net every 10000 steps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### N-Steps 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Distributional RL with 51 Quantiles"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'#### Data'))\n",
    "display(Markdown(f'###### {config.PRELOADING_SIZE:,} records'))\n",
    "display(Markdown(f'###### {config.src_vocab_size:,} src vocabulary size'))\n",
    "display(Markdown(f'###### {config.tgt_vocab_size:,} tgt vocabulary size'))\n",
    "\n",
    "display(Markdown(f'#### Vocabulary'))\n",
    "display(Markdown(\"###### [\"+\"] [\".join([voc[0].replace('<', '&lt;').replace('>', '&gt;') for voc in tgt_vocab.stoi.items() if voc[1] != 0]) + \"]\"))\n",
    "\n",
    "display(Markdown(f'#### seq2seq - Hyperparameter'))\n",
    "display(Markdown(f'###### Embedding Size: {config.emb_size}'))\n",
    "display(Markdown(f'###### RNN Size: {config.rnn_size}'))\n",
    "if config.SUPERVISED_PRETRAINING: display(Markdown(f'###### Pretrain Model'))\n",
    "\n",
    "display(Markdown(f'#### RL - Hyperparameter'))\n",
    "display(Markdown(f'###### Update Target Net every {config.target_update_freq} steps'))\n",
    "display(Markdown(f'###### N-Steps {config.N_STEPS}'))\n",
    "if config.DISTRIBUTIONAL: display(Markdown(f'###### Distributional RL with {config.QUANTILES} Quantiles'))\n",
    "if config.value_penalty: display(Markdown(f'###### Using value penalty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq-DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/DQN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config, DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (encoder_embeddings): Embeddings(\n",
       "    (make_embedding): Sequential(\n",
       "      (emb_luts): Elementwise(\n",
       "        (0): Embedding(111, 100, padding_idx=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): RNNEncoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (make_embedding): Sequential(\n",
       "        (emb_luts): Elementwise(\n",
       "          (0): Embedding(111, 100, padding_idx=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rnn): GRU(100, 250, bidirectional=True)\n",
       "  )\n",
       "  (decoder_embeddings): Embeddings(\n",
       "    (make_embedding): Sequential(\n",
       "      (emb_luts): Elementwise(\n",
       "        (0): Embedding(108, 100, padding_idx=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): InputFeedRNNDecoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (make_embedding): Sequential(\n",
       "        (emb_luts): Elementwise(\n",
       "          (0): Embedding(108, 100, padding_idx=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0)\n",
       "    (rnn): StackedGRU(\n",
       "      (dropout): Dropout(p=0.0)\n",
       "      (layers): ModuleList(\n",
       "        (0): GRUCell(600, 500)\n",
       "      )\n",
       "    )\n",
       "    (attn): GlobalAttention(\n",
       "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
       "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (advantages): Sequential(\n",
       "      (0): NoisyLinear()\n",
       "      (1): Mish()\n",
       "      (2): NoisyLinear()\n",
       "    )\n",
       "    (value): Sequential(\n",
       "      (0): NoisyLinear()\n",
       "      (1): Mish()\n",
       "      (2): NoisyLinear()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.current_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Total Number of Parameters: 9,525,018"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.current_model.parameters() if p.requires_grad)\n",
    "display(Markdown(f'##### Total Number of Parameters: {total_params:,}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/MSELoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MSELoss(\n",
    "    #nn.MSELoss(reduction=\"none\"),\n",
    "    nn.SmoothL1Loss(reduction=\"none\"),\n",
    "    model.current_model.generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/Reward.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rewards = ['BLEU']\n",
    "config.rewards_weights = [1]    \n",
    "\n",
    "reward = Reward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_optimizer = torch.optim.Adam(model.current_model.parameters(), lr=config.LR)\n",
    "from lib.Ranger import Ranger\n",
    "torch_optimizer = Ranger(model.current_model.parameters(), lr=config.LR)\n",
    "optim = onmt.utils.optimizers.Optimizer(torch_optimizer, learning_rate=config.LR, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_manager = onmt.utils.ReportMgr(report_every=1, start_time=None, tensorboard_writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/RLModelSaver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saver = RLModelSaver(\"checkpoints/checkpoint\", model, config, vocab_fields, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.Random(42).shuffle(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload Experience Replay Buffer\n",
    "if len(model.replay_memory) == 0:\n",
    "    for example in filtered_data[150:]:\n",
    "        model.replay_memory.preload(example.src[0].squeeze(1), example.tgt.squeeze(1), 1)\n",
    "        model.sample_buffer.preload(example.src[0].squeeze(1), example.tgt.squeeze(1), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/QLearning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QLearning(config,\n",
    "                    model,\n",
    "                    reward=reward,\n",
    "                    train_loss=loss,\n",
    "                    valid_loss=loss,\n",
    "                    optim=optim,\n",
    "                    model_saver = model_saver)\n",
    "                    #shard_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Examples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why are you here ?  ||  <s> I don &apos;t know . </s>\n",
      "Oh , no !  ||  <s> Oh , no ! </s>\n",
      "You want that ?  ||  <s> Yes . </s>\n",
      "Why did he ?  ||  <s> Why did he what ? </s>\n",
      "Oh -- Oh --  ||  <s> Oh -- are you -- are you all right ? </s>\n",
      "What do you think it is ?  ||  <s> What do you think it is ? </s>\n",
      "Where ?  ||  <s> Just ... I have to . </s>\n",
      "Yeah . You ?  ||  <s> Yeah . </s>\n",
      "No .  ||  <s> I , I just can &apos;t ... </s>\n",
      "What are you doing ?  ||  <s> What ? </s>\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(f'##### Examples'))\n",
    "for i, x in enumerate(filtered_data[0:10]):\n",
    "    print(' '.join([src_vocab.itos[token] for token in x.src[0].squeeze().tolist()]) + '  ||  ' + ' '.join([tgt_vocab.itos[token] for token in x.tgt.squeeze().tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.SAVE_PRETRAIN_SAMPLE_EVERY = 6\n",
    "config.SAVE_SAMPLE_EVERY = 1\n",
    "config.SAVE_SIGMA_EVERY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.SUPERVISED_PRETRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start pretraining - training loop\n",
      "INFO:root:Pretraining Step 1\n",
      "INFO:root:Pretraining Step 2\n",
      "INFO:root:Pretraining Step 3\n",
      "INFO:root:Pretraining Step 4\n",
      "INFO:root:Pretraining Step 5\n",
      "INFO:root:Pretraining Step 6\n",
      "INFO:root:Pretraining Step 7\n",
      "INFO:root:Pretraining Step 8\n",
      "INFO:root:Pretraining Step 9\n",
      "INFO:root:Pretraining Step 10\n",
      "INFO:root:Pretraining Step 11\n",
      "INFO:root:Pretraining Step 12\n",
      "INFO:root:Pretraining Step 13\n",
      "INFO:root:Pretraining Step 14\n",
      "INFO:root:Pretraining Step 15\n",
      "INFO:root:Pretraining Step 16\n",
      "INFO:root:Pretraining Step 17\n",
      "INFO:root:Pretraining Step 18\n",
      "INFO:root:Pretraining Step 19\n",
      "INFO:root:Pretraining Step 20\n",
      "INFO:root:Pretraining Step 21\n",
      "INFO:root:Pretraining Step 22\n",
      "INFO:root:Pretraining Step 23\n",
      "INFO:root:Pretraining Step 24\n",
      "INFO:root:Pretraining Step 25\n",
      "INFO:root:Pretraining Step 26\n",
      "INFO:root:Pretraining Step 27\n",
      "INFO:root:Pretraining Step 28\n",
      "INFO:root:Pretraining Step 29\n",
      "INFO:root:Pretraining Step 30\n",
      "INFO:root:Pretraining Step 31\n",
      "INFO:root:Pretraining Step 32\n",
      "INFO:root:Pretraining Step 33\n",
      "INFO:root:Pretraining Step 34\n",
      "INFO:root:Pretraining Step 35\n",
      "INFO:root:Pretraining Step 36\n",
      "INFO:root:Pretraining Step 37\n",
      "INFO:root:Pretraining Step 38\n",
      "INFO:root:Pretraining Step 39\n",
      "INFO:root:Pretraining Step 40\n",
      "INFO:root:Pretraining Step 41\n",
      "INFO:root:Pretraining Step 42\n",
      "INFO:root:Pretraining Step 43\n",
      "INFO:root:Pretraining Step 44\n",
      "INFO:root:Pretraining Step 45\n",
      "INFO:root:Pretraining Step 46\n",
      "INFO:root:Pretraining Step 47\n",
      "INFO:root:Pretraining Step 48\n",
      "INFO:root:Pretraining Step 49\n",
      "INFO:root:Pretraining Step 50\n",
      "INFO:root:Pretraining Step 51\n",
      "INFO:root:Pretraining Step 52\n",
      "INFO:root:Pretraining Step 53\n",
      "INFO:root:Pretraining Step 54\n",
      "INFO:root:Pretraining Step 55\n",
      "INFO:root:Pretraining Step 56\n",
      "INFO:root:Pretraining Step 57\n",
      "INFO:root:Pretraining Step 58\n",
      "INFO:root:Pretraining Step 59\n",
      "INFO:root:Pretraining Step 60\n",
      "INFO:root:Pretraining Step 61\n",
      "INFO:root:Pretraining Step 62\n",
      "INFO:root:Pretraining Step 63\n",
      "INFO:root:Pretraining Step 64\n",
      "INFO:root:Pretraining Step 65\n",
      "INFO:root:Pretraining Step 66\n",
      "INFO:root:Pretraining Step 67\n",
      "INFO:root:Pretraining Step 68\n",
      "INFO:root:Pretraining Step 69\n",
      "INFO:root:Pretraining Step 70\n",
      "INFO:root:Pretraining Step 71\n",
      "INFO:root:Pretraining Step 72\n",
      "INFO:root:Pretraining Step 73\n",
      "INFO:root:Pretraining Step 74\n",
      "INFO:root:Pretraining Step 75\n",
      "INFO:root:Pretraining Step 76\n",
      "INFO:root:Pretraining Step 77\n",
      "INFO:root:Pretraining Step 78\n",
      "INFO:root:Pretraining Step 79\n",
      "INFO:root:Pretraining Step 80\n",
      "INFO:root:Pretraining Step 81\n",
      "INFO:root:Pretraining Step 82\n",
      "INFO:root:Pretraining Step 83\n",
      "INFO:root:Pretraining Step 84\n",
      "INFO:root:Pretraining Step 85\n",
      "INFO:root:Pretraining Step 86\n",
      "INFO:root:Pretraining Step 87\n",
      "INFO:root:Pretraining Step 88\n",
      "INFO:root:Pretraining Step 89\n",
      "INFO:root:Pretraining Step 90\n",
      "INFO:root:Pretraining Step 91\n",
      "INFO:root:Pretraining Step 92\n",
      "INFO:root:Pretraining Step 93\n",
      "INFO:root:Pretraining Step 94\n",
      "INFO:root:Pretraining Step 95\n",
      "INFO:root:Pretraining Step 96\n",
      "INFO:root:Pretraining Step 97\n",
      "INFO:root:Pretraining Step 98\n",
      "INFO:root:Pretraining Step 99\n",
      "INFO:root:Pretraining Step 100\n",
      "INFO:root:Saving model checkpoint checkpoints/pretrain_checkpoint_step_100.pt\n",
      "INFO:root:Pretraining Step 101\n",
      "INFO:root:Pretraining Step 102\n",
      "INFO:root:Pretraining Step 103\n",
      "INFO:root:Pretraining Step 104\n",
      "INFO:root:Pretraining Step 105\n",
      "INFO:root:Pretraining Step 106\n",
      "INFO:root:Pretraining Step 107\n",
      "INFO:root:Pretraining Step 108\n",
      "INFO:root:Pretraining Step 109\n",
      "INFO:root:Pretraining Step 110\n",
      "INFO:root:Pretraining Step 111\n",
      "INFO:root:Pretraining Step 112\n",
      "INFO:root:Pretraining Step 113\n",
      "INFO:root:Pretraining Step 114\n",
      "INFO:root:Pretraining Step 115\n",
      "INFO:root:Pretraining Step 116\n",
      "INFO:root:Pretraining Step 117\n",
      "INFO:root:Pretraining Step 118\n",
      "INFO:root:Pretraining Step 119\n",
      "INFO:root:Pretraining Step 120\n",
      "INFO:root:Pretraining Step 121\n",
      "INFO:root:Pretraining Step 122\n",
      "INFO:root:Pretraining Step 123\n",
      "INFO:root:Pretraining Step 124\n",
      "INFO:root:Pretraining Step 125\n",
      "INFO:root:Pretraining Step 126\n",
      "INFO:root:Pretraining Step 127\n",
      "INFO:root:Pretraining Step 128\n",
      "INFO:root:Pretraining Step 129\n",
      "INFO:root:Pretraining Step 130\n",
      "INFO:root:Pretraining Step 131\n",
      "INFO:root:Pretraining Step 132\n",
      "INFO:root:Pretraining Step 133\n",
      "INFO:root:Pretraining Step 134\n",
      "INFO:root:Pretraining Step 135\n",
      "INFO:root:Pretraining Step 136\n",
      "INFO:root:Pretraining Step 137\n",
      "INFO:root:Pretraining Step 138\n",
      "INFO:root:Pretraining Step 139\n",
      "INFO:root:Pretraining Step 140\n",
      "INFO:root:Pretraining Step 141\n",
      "INFO:root:Pretraining Step 142\n",
      "INFO:root:Pretraining Step 143\n",
      "INFO:root:Pretraining Step 144\n",
      "INFO:root:Pretraining Step 145\n",
      "INFO:root:Pretraining Step 146\n",
      "INFO:root:Pretraining Step 147\n",
      "INFO:root:Pretraining Step 148\n",
      "INFO:root:Pretraining Step 149\n",
      "INFO:root:Pretraining Step 150\n",
      "INFO:root:Pretraining Step 151\n",
      "INFO:root:Pretraining Step 152\n",
      "INFO:root:Pretraining Step 153\n",
      "INFO:root:Pretraining Step 154\n",
      "INFO:root:Pretraining Step 155\n",
      "INFO:root:Pretraining Step 156\n",
      "INFO:root:Pretraining Step 157\n",
      "INFO:root:Pretraining Step 158\n",
      "INFO:root:Pretraining Step 159\n",
      "INFO:root:Pretraining Step 160\n",
      "INFO:root:Pretraining Step 161\n",
      "INFO:root:Pretraining Step 162\n",
      "INFO:root:Pretraining Step 163\n",
      "INFO:root:Pretraining Step 164\n",
      "INFO:root:Pretraining Step 165\n",
      "INFO:root:Pretraining Step 166\n",
      "INFO:root:Pretraining Step 167\n",
      "INFO:root:Pretraining Step 168\n",
      "INFO:root:Pretraining Step 169\n",
      "INFO:root:Pretraining Step 170\n",
      "INFO:root:Pretraining Step 171\n",
      "INFO:root:Pretraining Step 172\n",
      "INFO:root:Pretraining Step 173\n",
      "INFO:root:Pretraining Step 174\n",
      "INFO:root:Pretraining Step 175\n",
      "INFO:root:Pretraining Step 176\n",
      "INFO:root:Pretraining Step 177\n",
      "INFO:root:Pretraining Step 178\n",
      "INFO:root:Pretraining Step 179\n",
      "INFO:root:Pretraining Step 180\n",
      "INFO:root:Pretraining Step 181\n",
      "INFO:root:Pretraining Step 182\n",
      "INFO:root:Pretraining Step 183\n",
      "INFO:root:Pretraining Step 184\n",
      "INFO:root:Pretraining Step 185\n",
      "INFO:root:Pretraining Step 186\n",
      "INFO:root:Pretraining Step 187\n",
      "INFO:root:Pretraining Step 188\n",
      "INFO:root:Pretraining Step 189\n",
      "INFO:root:Pretraining Step 190\n",
      "INFO:root:Pretraining Step 191\n",
      "INFO:root:Pretraining Step 192\n",
      "INFO:root:Pretraining Step 193\n",
      "INFO:root:Pretraining Step 194\n",
      "INFO:root:Pretraining Step 195\n",
      "INFO:root:Pretraining Step 196\n",
      "INFO:root:Pretraining Step 197\n",
      "INFO:root:Pretraining Step 198\n",
      "INFO:root:Pretraining Step 199\n",
      "INFO:root:Pretraining Step 200\n",
      "INFO:root:Saving model checkpoint checkpoints/pretrain_checkpoint_step_200.pt\n",
      "INFO:root:Pretraining Step 201\n",
      "INFO:root:Pretraining Step 202\n",
      "INFO:root:Pretraining Step 203\n",
      "INFO:root:Pretraining Step 204\n",
      "INFO:root:Pretraining Step 205\n",
      "INFO:root:Pretraining Step 206\n",
      "INFO:root:Pretraining Step 207\n",
      "INFO:root:Pretraining Step 208\n",
      "INFO:root:Pretraining Step 209\n",
      "INFO:root:Pretraining Step 210\n",
      "INFO:root:Pretraining Step 211\n",
      "INFO:root:Pretraining Step 212\n",
      "INFO:root:Pretraining Step 213\n",
      "INFO:root:Pretraining Step 214\n",
      "INFO:root:Pretraining Step 215\n",
      "INFO:root:Pretraining Step 216\n",
      "INFO:root:Pretraining Step 217\n",
      "INFO:root:Pretraining Step 218\n",
      "INFO:root:Pretraining Step 219\n",
      "INFO:root:Pretraining Step 220\n",
      "INFO:root:Pretraining Step 221\n",
      "INFO:root:Pretraining Step 222\n",
      "INFO:root:Pretraining Step 223\n",
      "INFO:root:Pretraining Step 224\n",
      "INFO:root:Pretraining Step 225\n",
      "INFO:root:Pretraining Step 226\n",
      "INFO:root:Pretraining Step 227\n",
      "INFO:root:Pretraining Step 228\n",
      "INFO:root:Pretraining Step 229\n",
      "INFO:root:Pretraining Step 230\n",
      "INFO:root:Pretraining Step 231\n",
      "INFO:root:Pretraining Step 232\n",
      "INFO:root:Pretraining Step 233\n",
      "INFO:root:Pretraining Step 234\n",
      "INFO:root:Pretraining Step 235\n",
      "INFO:root:Pretraining Step 236\n",
      "INFO:root:Pretraining Step 237\n",
      "INFO:root:Pretraining Step 238\n",
      "INFO:root:Pretraining Step 239\n",
      "INFO:root:Pretraining Step 240\n",
      "INFO:root:Pretraining Step 241\n",
      "INFO:root:Pretraining Step 242\n",
      "INFO:root:Pretraining Step 243\n",
      "INFO:root:Pretraining Step 244\n",
      "INFO:root:Pretraining Step 245\n",
      "INFO:root:Pretraining Step 246\n",
      "INFO:root:Pretraining Step 247\n",
      "INFO:root:Pretraining Step 248\n",
      "INFO:root:Pretraining Step 249\n",
      "INFO:root:Pretraining Step 250\n",
      "INFO:root:Pretraining Step 251\n",
      "INFO:root:Pretraining Step 252\n",
      "INFO:root:Pretraining Step 253\n",
      "INFO:root:Pretraining Step 254\n",
      "INFO:root:Pretraining Step 255\n",
      "INFO:root:Pretraining Step 256\n",
      "INFO:root:Pretraining Step 257\n",
      "INFO:root:Pretraining Step 258\n",
      "INFO:root:Pretraining Step 259\n",
      "INFO:root:Pretraining Step 260\n",
      "INFO:root:Pretraining Step 261\n",
      "INFO:root:Pretraining Step 262\n",
      "INFO:root:Pretraining Step 263\n",
      "INFO:root:Pretraining Step 264\n",
      "INFO:root:Pretraining Step 265\n",
      "INFO:root:Pretraining Step 266\n",
      "INFO:root:Pretraining Step 267\n",
      "INFO:root:Pretraining Step 268\n",
      "INFO:root:Pretraining Step 269\n",
      "INFO:root:Pretraining Step 270\n",
      "INFO:root:Pretraining Step 271\n",
      "INFO:root:Pretraining Step 272\n",
      "INFO:root:Pretraining Step 273\n",
      "INFO:root:Pretraining Step 274\n",
      "INFO:root:Pretraining Step 275\n",
      "INFO:root:Pretraining Step 276\n",
      "INFO:root:Pretraining Step 277\n",
      "INFO:root:Pretraining Step 278\n",
      "INFO:root:Pretraining Step 279\n",
      "INFO:root:Pretraining Step 280\n",
      "INFO:root:Pretraining Step 281\n",
      "INFO:root:Pretraining Step 282\n",
      "INFO:root:Pretraining Step 283\n",
      "INFO:root:Pretraining Step 284\n",
      "INFO:root:Pretraining Step 285\n",
      "INFO:root:Pretraining Step 286\n",
      "INFO:root:Pretraining Step 287\n",
      "INFO:root:Pretraining Step 288\n",
      "INFO:root:Pretraining Step 289\n",
      "INFO:root:Pretraining Step 290\n",
      "INFO:root:Pretraining Step 291\n",
      "INFO:root:Pretraining Step 292\n",
      "INFO:root:Pretraining Step 293\n",
      "INFO:root:Pretraining Step 294\n",
      "INFO:root:Pretraining Step 295\n",
      "INFO:root:Pretraining Step 296\n",
      "INFO:root:Pretraining Step 297\n",
      "INFO:root:Pretraining Step 298\n",
      "INFO:root:Pretraining Step 299\n",
      "INFO:root:Pretraining Step 300\n",
      "INFO:root:Saving model checkpoint checkpoints/pretrain_checkpoint_step_300.pt\n",
      "INFO:root:Pretraining Step 301\n",
      "INFO:root:Pretraining Step 302\n",
      "INFO:root:Pretraining Step 303\n",
      "INFO:root:Pretraining Step 304\n",
      "INFO:root:Pretraining Step 305\n",
      "INFO:root:Pretraining Step 306\n",
      "INFO:root:Pretraining Step 307\n",
      "INFO:root:Pretraining Step 308\n",
      "INFO:root:Pretraining Step 309\n",
      "INFO:root:Pretraining Step 310\n",
      "INFO:root:Pretraining Step 311\n",
      "INFO:root:Pretraining Step 312\n",
      "INFO:root:Pretraining Step 313\n",
      "INFO:root:Pretraining Step 314\n",
      "INFO:root:Pretraining Step 315\n",
      "INFO:root:Pretraining Step 316\n",
      "INFO:root:Pretraining Step 317\n",
      "INFO:root:Pretraining Step 318\n",
      "INFO:root:Pretraining Step 319\n",
      "INFO:root:Pretraining Step 320\n",
      "INFO:root:Pretraining Step 321\n",
      "INFO:root:Pretraining Step 322\n",
      "INFO:root:Pretraining Step 323\n",
      "INFO:root:Pretraining Step 324\n",
      "INFO:root:Pretraining Step 325\n",
      "INFO:root:Pretraining Step 326\n",
      "INFO:root:Pretraining Step 327\n",
      "INFO:root:Pretraining Step 328\n",
      "INFO:root:Pretraining Step 329\n",
      "INFO:root:Pretraining Step 330\n",
      "INFO:root:Pretraining Step 331\n",
      "INFO:root:Pretraining Step 332\n",
      "INFO:root:Pretraining Step 333\n",
      "INFO:root:Pretraining Step 334\n",
      "INFO:root:Pretraining Step 335\n",
      "INFO:root:Pretraining Step 336\n",
      "INFO:root:Pretraining Step 337\n",
      "INFO:root:Pretraining Step 338\n",
      "INFO:root:Pretraining Step 339\n",
      "INFO:root:Pretraining Step 340\n",
      "INFO:root:Pretraining Step 341\n",
      "INFO:root:Pretraining Step 342\n",
      "INFO:root:Pretraining Step 343\n",
      "INFO:root:Pretraining Step 344\n",
      "INFO:root:Pretraining Step 345\n",
      "INFO:root:Pretraining Step 346\n",
      "INFO:root:Pretraining Step 347\n",
      "INFO:root:Pretraining Step 348\n",
      "INFO:root:Pretraining Step 349\n",
      "INFO:root:Pretraining Step 350\n",
      "INFO:root:Pretraining Step 351\n",
      "INFO:root:Pretraining Step 352\n",
      "INFO:root:Pretraining Step 353\n",
      "INFO:root:Pretraining Step 354\n",
      "INFO:root:Pretraining Step 355\n",
      "INFO:root:Pretraining Step 356\n",
      "INFO:root:Pretraining Step 357\n",
      "INFO:root:Pretraining Step 358\n",
      "INFO:root:Pretraining Step 359\n",
      "INFO:root:Pretraining Step 360\n",
      "INFO:root:Pretraining Step 361\n",
      "INFO:root:Pretraining Step 362\n",
      "INFO:root:Pretraining Step 363\n",
      "INFO:root:Pretraining Step 364\n",
      "INFO:root:Pretraining Step 365\n",
      "INFO:root:Pretraining Step 366\n",
      "INFO:root:Pretraining Step 367\n",
      "INFO:root:Pretraining Step 368\n",
      "INFO:root:Pretraining Step 369\n",
      "INFO:root:Pretraining Step 370\n",
      "INFO:root:Pretraining Step 371\n",
      "INFO:root:Pretraining Step 372\n",
      "INFO:root:Pretraining Step 373\n",
      "INFO:root:Pretraining Step 374\n",
      "INFO:root:Pretraining Step 375\n",
      "INFO:root:Pretraining Step 376\n",
      "INFO:root:Pretraining Step 377\n",
      "INFO:root:Pretraining Step 378\n",
      "INFO:root:Pretraining Step 379\n",
      "INFO:root:Pretraining Step 380\n",
      "INFO:root:Pretraining Step 381\n",
      "INFO:root:Pretraining Step 382\n",
      "INFO:root:Pretraining Step 383\n",
      "INFO:root:Pretraining Step 384\n",
      "INFO:root:Pretraining Step 385\n",
      "INFO:root:Pretraining Step 386\n",
      "INFO:root:Pretraining Step 387\n",
      "INFO:root:Pretraining Step 388\n",
      "INFO:root:Pretraining Step 389\n",
      "INFO:root:Pretraining Step 390\n",
      "INFO:root:Pretraining Step 391\n",
      "INFO:root:Pretraining Step 392\n",
      "INFO:root:Pretraining Step 393\n",
      "INFO:root:Pretraining Step 394\n",
      "INFO:root:Pretraining Step 395\n",
      "INFO:root:Pretraining Step 396\n",
      "INFO:root:Pretraining Step 397\n",
      "INFO:root:Pretraining Step 398\n",
      "INFO:root:Pretraining Step 399\n",
      "INFO:root:Pretraining Step 400\n",
      "INFO:root:Saving model checkpoint checkpoints/pretrain_checkpoint_step_400.pt\n",
      "INFO:root:Pretraining Step 401\n",
      "INFO:root:Pretraining Step 402\n",
      "INFO:root:Pretraining Step 403\n",
      "INFO:root:Pretraining Step 404\n",
      "INFO:root:Pretraining Step 405\n",
      "INFO:root:Pretraining Step 406\n",
      "INFO:root:Pretraining Step 407\n",
      "INFO:root:Pretraining Step 408\n",
      "INFO:root:Pretraining Step 409\n",
      "INFO:root:Pretraining Step 410\n",
      "INFO:root:Pretraining Step 411\n",
      "INFO:root:Pretraining Step 412\n",
      "INFO:root:Pretraining Step 413\n",
      "INFO:root:Pretraining Step 414\n",
      "INFO:root:Pretraining Step 415\n",
      "INFO:root:Pretraining Step 416\n",
      "INFO:root:Pretraining Step 417\n",
      "INFO:root:Pretraining Step 418\n",
      "INFO:root:Pretraining Step 419\n",
      "INFO:root:Pretraining Step 420\n",
      "INFO:root:Pretraining Step 421\n",
      "INFO:root:Pretraining Step 422\n",
      "INFO:root:Pretraining Step 423\n",
      "INFO:root:Pretraining Step 424\n",
      "INFO:root:Pretraining Step 425\n",
      "INFO:root:Pretraining Step 426\n",
      "INFO:root:Pretraining Step 427\n",
      "INFO:root:Pretraining Step 428\n",
      "INFO:root:Pretraining Step 429\n",
      "INFO:root:Pretraining Step 430\n",
      "INFO:root:Pretraining Step 431\n",
      "INFO:root:Pretraining Step 432\n",
      "INFO:root:Pretraining Step 433\n",
      "INFO:root:Pretraining Step 434\n",
      "INFO:root:Pretraining Step 435\n",
      "INFO:root:Pretraining Step 436\n",
      "INFO:root:Pretraining Step 437\n",
      "INFO:root:Pretraining Step 438\n",
      "INFO:root:Pretraining Step 439\n",
      "INFO:root:Pretraining Step 440\n",
      "INFO:root:Pretraining Step 441\n",
      "INFO:root:Pretraining Step 442\n",
      "INFO:root:Pretraining Step 443\n",
      "INFO:root:Pretraining Step 444\n",
      "INFO:root:Pretraining Step 445\n",
      "INFO:root:Pretraining Step 446\n",
      "INFO:root:Pretraining Step 447\n",
      "INFO:root:Pretraining Step 448\n",
      "INFO:root:Pretraining Step 449\n",
      "INFO:root:Pretraining Step 450\n",
      "INFO:root:Pretraining Step 451\n",
      "INFO:root:Pretraining Step 452\n",
      "INFO:root:Pretraining Step 453\n",
      "INFO:root:Pretraining Step 454\n",
      "INFO:root:Pretraining Step 455\n",
      "INFO:root:Pretraining Step 456\n",
      "INFO:root:Pretraining Step 457\n",
      "INFO:root:Pretraining Step 458\n",
      "INFO:root:Pretraining Step 459\n",
      "INFO:root:Pretraining Step 460\n",
      "INFO:root:Pretraining Step 461\n",
      "INFO:root:Pretraining Step 462\n",
      "INFO:root:Pretraining Step 463\n",
      "INFO:root:Pretraining Step 464\n",
      "INFO:root:Pretraining Step 465\n",
      "INFO:root:Pretraining Step 466\n",
      "INFO:root:Pretraining Step 467\n",
      "INFO:root:Pretraining Step 468\n",
      "INFO:root:Pretraining Step 469\n",
      "INFO:root:Pretraining Step 470\n",
      "INFO:root:Pretraining Step 471\n",
      "INFO:root:Pretraining Step 472\n",
      "INFO:root:Pretraining Step 473\n",
      "INFO:root:Pretraining Step 474\n",
      "INFO:root:Pretraining Step 475\n",
      "INFO:root:Pretraining Step 476\n",
      "INFO:root:Pretraining Step 477\n",
      "INFO:root:Pretraining Step 478\n",
      "INFO:root:Pretraining Step 479\n",
      "INFO:root:Pretraining Step 480\n",
      "INFO:root:Pretraining Step 481\n",
      "INFO:root:Pretraining Step 482\n",
      "INFO:root:Pretraining Step 483\n",
      "INFO:root:Pretraining Step 484\n",
      "INFO:root:Pretraining Step 485\n",
      "INFO:root:Pretraining Step 486\n",
      "INFO:root:Pretraining Step 487\n",
      "INFO:root:Pretraining Step 488\n",
      "INFO:root:Pretraining Step 489\n",
      "INFO:root:Pretraining Step 490\n",
      "INFO:root:Pretraining Step 491\n",
      "INFO:root:Pretraining Step 492\n",
      "INFO:root:Pretraining Step 493\n",
      "INFO:root:Pretraining Step 494\n",
      "INFO:root:Pretraining Step 495\n",
      "INFO:root:Pretraining Step 496\n",
      "INFO:root:Pretraining Step 497\n",
      "INFO:root:Pretraining Step 498\n",
      "INFO:root:Pretraining Step 499\n",
      "INFO:root:Pretraining Step 500\n",
      "INFO:root:Saving model checkpoint checkpoints/pretrain_checkpoint_step_500.pt\n"
     ]
    }
   ],
   "source": [
    "if config.SUPERVISED_PRETRAINING:\n",
    "    trainer.pretrain(train_steps=500, save_checkpoint_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start q-learning training loop\n",
      "INFO:root:Q-Learning Step 1\n",
      "INFO:root:Q-Learning Step 2\n",
      "INFO:root:Q-Learning Step 3\n",
      "INFO:root:Q-Learning Step 4\n",
      "INFO:root:Q-Learning Step 5\n",
      "INFO:root:Q-Learning Step 6\n",
      "INFO:root:Q-Learning Step 7\n",
      "INFO:root:Q-Learning Step 8\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 9\n",
      "INFO:root:Q-Learning Step 10\n",
      "INFO:root:Q-Learning Step 11\n",
      "INFO:root:Q-Learning Step 12\n",
      "INFO:root:Q-Learning Step 13\n",
      "INFO:root:Q-Learning Step 14\n",
      "INFO:root:Q-Learning Step 15\n",
      "INFO:root:Q-Learning Step 16\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 17\n",
      "INFO:root:Q-Learning Step 18\n",
      "INFO:root:Q-Learning Step 19\n",
      "INFO:root:Q-Learning Step 20\n",
      "INFO:root:Q-Learning Step 21\n",
      "INFO:root:Q-Learning Step 22\n",
      "INFO:root:Q-Learning Step 23\n",
      "INFO:root:Q-Learning Step 24\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 25\n",
      "INFO:root:Q-Learning Step 26\n",
      "INFO:root:Q-Learning Step 27\n",
      "INFO:root:Q-Learning Step 28\n",
      "INFO:root:Q-Learning Step 29\n",
      "INFO:root:Q-Learning Step 30\n",
      "INFO:root:Q-Learning Step 31\n",
      "INFO:root:Q-Learning Step 32\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 33\n",
      "INFO:root:Q-Learning Step 34\n",
      "INFO:root:Q-Learning Step 35\n",
      "INFO:root:Q-Learning Step 36\n",
      "INFO:root:Q-Learning Step 37\n",
      "INFO:root:Q-Learning Step 38\n",
      "INFO:root:Q-Learning Step 39\n",
      "INFO:root:Q-Learning Step 40\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 41\n",
      "INFO:root:Q-Learning Step 42\n",
      "INFO:root:Q-Learning Step 43\n",
      "INFO:root:Q-Learning Step 44\n",
      "INFO:root:Q-Learning Step 45\n",
      "INFO:root:Q-Learning Step 46\n",
      "INFO:root:Q-Learning Step 47\n",
      "INFO:root:Q-Learning Step 48\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 49\n",
      "INFO:root:Q-Learning Step 50\n",
      "INFO:root:Saving model checkpoint checkpoints/checkpoint_step_50.pt\n",
      "INFO:root:Saving replay checkpoint checkpoints/checkpoint_recent_replay.pt\n",
      "INFO:root:Q-Learning Step 51\n",
      "INFO:root:Q-Learning Step 52\n",
      "INFO:root:Q-Learning Step 53\n",
      "INFO:root:Q-Learning Step 54\n",
      "INFO:root:Q-Learning Step 55\n",
      "INFO:root:Q-Learning Step 56\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 57\n",
      "INFO:root:Q-Learning Step 58\n",
      "INFO:root:Q-Learning Step 59\n",
      "INFO:root:Q-Learning Step 60\n",
      "INFO:root:Q-Learning Step 61\n",
      "INFO:root:Q-Learning Step 62\n",
      "INFO:root:Q-Learning Step 63\n",
      "INFO:root:Q-Learning Step 64\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 65\n",
      "INFO:root:Q-Learning Step 66\n",
      "INFO:root:Q-Learning Step 67\n",
      "INFO:root:Q-Learning Step 68\n",
      "INFO:root:Q-Learning Step 69\n",
      "INFO:root:Q-Learning Step 70\n",
      "INFO:root:Q-Learning Step 71\n",
      "INFO:root:Q-Learning Step 72\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 73\n",
      "INFO:root:Q-Learning Step 74\n",
      "INFO:root:Q-Learning Step 75\n",
      "INFO:root:Q-Learning Step 76\n",
      "INFO:root:Q-Learning Step 77\n",
      "INFO:root:Q-Learning Step 78\n",
      "INFO:root:Q-Learning Step 79\n",
      "INFO:root:Q-Learning Step 80\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 81\n",
      "INFO:root:Q-Learning Step 82\n",
      "INFO:root:Q-Learning Step 83\n",
      "INFO:root:Q-Learning Step 84\n",
      "INFO:root:Q-Learning Step 85\n",
      "INFO:root:Q-Learning Step 86\n",
      "INFO:root:Q-Learning Step 87\n",
      "INFO:root:Q-Learning Step 88\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 89\n",
      "INFO:root:Q-Learning Step 90\n",
      "INFO:root:Q-Learning Step 91\n",
      "INFO:root:Q-Learning Step 92\n",
      "INFO:root:Q-Learning Step 93\n",
      "INFO:root:Q-Learning Step 94\n",
      "INFO:root:Q-Learning Step 95\n",
      "INFO:root:Q-Learning Step 96\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 97\n",
      "INFO:root:Q-Learning Step 98\n",
      "INFO:root:Q-Learning Step 99\n",
      "INFO:root:Q-Learning Step 100\n",
      "INFO:root:Saving model checkpoint checkpoints/checkpoint_step_100.pt\n",
      "INFO:root:Saving replay checkpoint checkpoints/checkpoint_recent_replay.pt\n",
      "INFO:root:Q-Learning Step 101\n",
      "INFO:root:Q-Learning Step 102\n",
      "INFO:root:Q-Learning Step 103\n",
      "INFO:root:Q-Learning Step 104\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 105\n",
      "INFO:root:Q-Learning Step 106\n",
      "INFO:root:Q-Learning Step 107\n",
      "INFO:root:Q-Learning Step 108\n",
      "INFO:root:Q-Learning Step 109\n",
      "INFO:root:Q-Learning Step 110\n",
      "INFO:root:Q-Learning Step 111\n",
      "INFO:root:Q-Learning Step 112\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 113\n",
      "INFO:root:Q-Learning Step 114\n",
      "INFO:root:Q-Learning Step 115\n",
      "INFO:root:Q-Learning Step 116\n",
      "INFO:root:Q-Learning Step 117\n",
      "INFO:root:Q-Learning Step 118\n",
      "INFO:root:Q-Learning Step 119\n",
      "INFO:root:Q-Learning Step 120\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 121\n",
      "INFO:root:Q-Learning Step 122\n",
      "INFO:root:Q-Learning Step 123\n",
      "INFO:root:Q-Learning Step 124\n",
      "INFO:root:Q-Learning Step 125\n",
      "INFO:root:Q-Learning Step 126\n",
      "INFO:root:Q-Learning Step 127\n",
      "INFO:root:Q-Learning Step 128\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 129\n",
      "INFO:root:Q-Learning Step 130\n",
      "INFO:root:Q-Learning Step 131\n",
      "INFO:root:Q-Learning Step 132\n",
      "INFO:root:Q-Learning Step 133\n",
      "INFO:root:Q-Learning Step 134\n",
      "INFO:root:Q-Learning Step 135\n",
      "INFO:root:Q-Learning Step 136\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 137\n",
      "INFO:root:Q-Learning Step 138\n",
      "INFO:root:Q-Learning Step 139\n",
      "INFO:root:Q-Learning Step 140\n",
      "INFO:root:Q-Learning Step 141\n",
      "INFO:root:Q-Learning Step 142\n",
      "INFO:root:Q-Learning Step 143\n",
      "INFO:root:Q-Learning Step 144\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Q-Learning Step 145\n",
      "INFO:root:Q-Learning Step 146\n",
      "INFO:root:Q-Learning Step 147\n",
      "INFO:root:Q-Learning Step 148\n",
      "INFO:root:Q-Learning Step 149\n",
      "INFO:root:Q-Learning Step 150\n",
      "INFO:root:Saving model checkpoint checkpoints/checkpoint_step_150.pt\n",
      "INFO:root:Saving replay checkpoint checkpoints/checkpoint_recent_replay.pt\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train(train_steps=150, save_checkpoint_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for para in list(model.current_model.parameters()):\n",
    "#    printa(para.grad.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Replay Memory\n",
    "#for i, x in enumerate(model.replay_memory._storage[2000:]):\n",
    "#    if ' '.join([tgt_vocab.itos[token] for token in x[1].squeeze().tolist()]) != '<s> </s>':\n",
    "#        print(i+2000, ' '.join([src_vocab.itos[token] for token in x[0].squeeze().tolist()]) + '  ||  ' + ' '.join([tgt_vocab.itos[token] for token in x[1].squeeze().tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up rewards in replay memory\n",
    "#for i in range(0,len(model.replay_memory),2000):\n",
    "#    sum_ = sum([y[2] for y in model.replay_memory._storage[i:i+2000]])\n",
    "#    print(i, i+2000, sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at priorities in prioritzed replay memory\n",
    "#for i in range(0,len(model.replay_memory),2000):\n",
    "#    print(i, i+2000, sum([model.replay_memory._it_sum[y] for y in range(i, i+2000)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

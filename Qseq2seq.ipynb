{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenizer Version 1.1', 'Language: en', 'Number of threads: 1']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!perl ../OpenNMT-py/tools/tokenizer.perl  -l en \\\n",
    "< ../data/\"cornell movie-dialogs corpus\"/src_movie_lines.txt \\\n",
    "> ../data/\"cornell movie-dialogs corpus\"/src_movie_lines_tok.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenizer Version 1.1', 'Language: en', 'Number of threads: 1']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!perl ../OpenNMT-py/tools/tokenizer.perl  -l en \\\n",
    "< ../data/\"cornell movie-dialogs corpus\"/tgt_movie_lines.txt \\\n",
    "> ../data/\"cornell movie-dialogs corpus\"/tgt_movie_lines_tok.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please backup existing pt files: ../data/cornell_raw.train*.pt, to avoid overwriting them!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please backup existing pt files: ../data/cornell_raw_min_30_10_tok.train*.pt, to avoid overwriting them!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw_min_30_10_tok --src_words_min_frequency 30 --tgt_words_min_frequency 30 --src_seq_length 10 --tgt_seq_length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2019-09-12 02:53:40,588 INFO] Extracting features...',\n",
       " '[2019-09-12 02:53:40,590 INFO]  * number of source features: 0.',\n",
       " '[2019-09-12 02:53:40,591 INFO]  * number of target features: 0.',\n",
       " '[2019-09-12 02:53:40,591 INFO] Building `Fields` object...',\n",
       " '[2019-09-12 02:53:40,591 INFO] Building & saving training data...',\n",
       " '[2019-09-12 02:53:40,591 INFO] Reading source and target files: ../data/cornell movie-dialogs corpus/src_movie_lines_tok.txt ../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt.',\n",
       " '[2019-09-12 02:53:40,770 INFO] Building shard 0.',\n",
       " '[2019-09-12 02:53:48,692 INFO]  * saving 0th train data shard to ../data/cornell_raw_min_100_tok.train.0.pt.',\n",
       " '[2019-09-12 02:53:49,922 INFO]  * tgt vocab size: 379.',\n",
       " '[2019-09-12 02:53:49,948 INFO]  * src vocab size: 380.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw_min_100_tok --src_words_min_frequency 100 --tgt_words_min_frequency 100 --src_seq_length 10 --tgt_seq_length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please backup existing pt files: ../data/cornell_raw_min_1000_tok.train*.pt, to avoid overwriting them!']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!python ../OpenNMT-py/preprocess.py --train_src \"data/cornell movie-dialogs corpus/src_movie_lines_tok.txt\" --train_tgt \"../data/cornell movie-dialogs corpus/tgt_movie_lines_tok.txt\" --save_data ../data/cornell_raw_min_1000_tok --src_words_min_frequency 1000 --tgt_words_min_frequency 1000 --src_seq_length 10 --tgt_seq_length 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fields = torch.load(\"data/cornell_raw_min_1000_tok.vocab.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token] #stoi: mapping token strings to numerical identifiers.\n",
    "# ['<unk>', '<blank>', 'I', 'you', 'the', 'to', 'a', 'of', 'and', 'You']\n",
    "# src_text_field.pad_token : '<blank>'\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab_size = len(src_vocab)\n",
    "config.tgt_vocab_size = len(tgt_vocab)\n",
    "config.src_padding = src_padding\n",
    "config.tgt_padding = tgt_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_unk = src_vocab.stoi[src_text_field.unk_token]\n",
    "config.tgt_unk = tgt_vocab.stoi[tgt_text_field.unk_token]\n",
    "config.tgt_bos = tgt_vocab.stoi[tgt_text_field.init_token]\n",
    "config.tgt_eos = tgt_vocab.stoi[tgt_text_field.eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.src_vocab = src_vocab\n",
    "config.tgt_vocab = tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "from itertools import chain\n",
    "\n",
    "train_data_file = \"data/cornell_raw_min_1000_tok.train.0.pt\"\n",
    "train_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[train_data_file],\n",
    "                                                     fields=vocab_fields,\n",
    "                                                     batch_size=1,\n",
    "                                                     batch_size_multiple=1,\n",
    "                                                     batch_size_fn=None,\n",
    "                                                     device=config.device,\n",
    "                                                     is_train=True,\n",
    "                                                     repeat=False,\n",
    "                                                     pool_factor=8192)\n",
    "\n",
    "data = list(train_iter)\n",
    "filtered_data = []\n",
    "for x in data:\n",
    "    # Filtering sentences with <unk> token\n",
    "    if not ((x.src[0].squeeze() == config.src_unk).any() or (x.tgt.squeeze() == config.tgt_unk).any()):\n",
    "        filtered_data.append(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.PRELOADING_SIZE = len(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### 492 records"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### 64 src vocabulary size"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### 61 tgt vocabulary size"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### seq2seq - Hyperparameter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Embedding Size: 100"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### RNN Size: 500"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### RL - Hyperparameter"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Update Target Net every 10000 steps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### Pretraining Iterations 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###### N-Steps 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'#### Data'))\n",
    "display(Markdown(f'###### {config.PRELOADING_SIZE:,} records'))\n",
    "display(Markdown(f'###### {config.src_vocab_size:,} src vocabulary size'))\n",
    "display(Markdown(f'###### {config.tgt_vocab_size:,} tgt vocabulary size'))\n",
    "\n",
    "display(Markdown(f'#### seq2seq - Hyperparameter'))\n",
    "display(Markdown(f'###### Embedding Size: {config.emb_size}'))\n",
    "display(Markdown(f'###### RNN Size: {config.rnn_size}'))\n",
    "\n",
    "display(Markdown(f'#### RL - Hyperparameter'))\n",
    "display(Markdown(f'###### Update Target Net every {config.target_update_freq} steps'))\n",
    "display(Markdown(f'###### Pretraining Iterations {config.PRETRAIN_ITER}'))\n",
    "display(Markdown(f'###### N-Steps {config.N_STEPS}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq-DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/DQN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config, DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (encoder_embeddings): Embeddings(\n",
       "    (make_embedding): Sequential(\n",
       "      (emb_luts): Elementwise(\n",
       "        (0): Embedding(64, 100, padding_idx=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): RNNEncoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (make_embedding): Sequential(\n",
       "        (emb_luts): Elementwise(\n",
       "          (0): Embedding(64, 100, padding_idx=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rnn): GRU(100, 250, bidirectional=True)\n",
       "  )\n",
       "  (decoder_embeddings): Embeddings(\n",
       "    (make_embedding): Sequential(\n",
       "      (emb_luts): Elementwise(\n",
       "        (0): Embedding(61, 100, padding_idx=1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): InputFeedRNNDecoder(\n",
       "    (embeddings): Embeddings(\n",
       "      (make_embedding): Sequential(\n",
       "        (emb_luts): Elementwise(\n",
       "          (0): Embedding(61, 100, padding_idx=1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0)\n",
       "    (rnn): StackedGRU(\n",
       "      (dropout): Dropout(p=0.0)\n",
       "      (layers): ModuleList(\n",
       "        (0): GRUCell(600, 500)\n",
       "      )\n",
       "    )\n",
       "    (attn): GlobalAttention(\n",
       "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
       "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (advantages): Sequential(\n",
       "      (0): NoisyLinear()\n",
       "      (1): ReLU()\n",
       "      (2): NoisyLinear()\n",
       "    )\n",
       "    (value): Sequential(\n",
       "      (0): NoisyLinear()\n",
       "      (1): ReLU()\n",
       "      (2): NoisyLinear()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.current_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Total Number of Parameters: 3,514,188"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.current_model.parameters() if p.requires_grad)\n",
    "display(Markdown(f'##### Total Number of Parameters: {total_params:,}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/MSELoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MSELoss(\n",
    "    #nn.MSELoss(reduction=\"none\"),\n",
    "    nn.SmoothL1Loss(reduction=\"none\"),\n",
    "    model.current_model.generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/Reward.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rewards = ['BLEU']\n",
    "config.rewards_weights = [1]    \n",
    "\n",
    "reward = Reward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_optimizer = torch.optim.Adam(model.current_model.parameters(), lr=config.LR)\n",
    "optim = onmt.utils.optimizers.Optimizer(torch_optimizer, learning_rate=config.LR, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_manager = onmt.utils.ReportMgr(report_every=1, start_time=None, tensorboard_writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/RLModelSaver.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saver = RLModelSaver(\"checkpoints/checkpoint\", model, config, vocab_fields, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload Experience Replay Buffer\n",
    "if len(model.replay_memory) == 0:\n",
    "    for example in filtered_data:\n",
    "        model.replay_memory.preload(example.src[0].squeeze(1), example.tgt.squeeze(1), 1)\n",
    "        model.sample_buffer.preload(example.src[0].squeeze(1), example.tgt.squeeze(1), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/QLearning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QLearning(config,\n",
    "                    model,\n",
    "                    reward=reward,\n",
    "                    train_loss=loss,\n",
    "                    valid_loss=loss,\n",
    "                    optim=optim,\n",
    "                    model_saver = model_saver)\n",
    "                    #shard_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Examples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No .  ||  <s> What do you want ? </s>\n",
      "Yeah .  ||  <s> Why ? </s>\n",
      "You are ?  ||  <s> Yeah . </s>\n",
      "No !  ||  <s> Yes ! </s>\n",
      "What &apos;s that ?  ||  <s> What ? </s>\n",
      "Why .  ||  <s> Why what ? </s>\n",
      "You know what for .  ||  <s> No I don &apos;t . </s>\n",
      "No .  ||  <s> Why not ? </s>\n",
      "I don &apos;t know .  ||  <s> You don &apos;t know . </s>\n",
      "Yeah I can &apos;t .  ||  <s> Why ? </s>\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(f'##### Examples'))\n",
    "for i, x in enumerate(filtered_data[0:10]):\n",
    "    print(' '.join([src_vocab.itos[token] for token in x.src[0].squeeze().tolist()]) + '  ||  ' + ' '.join([tgt_vocab.itos[token] for token in x.tgt.squeeze().tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.pretrain(500, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.current_model.encoder.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "#for param in model.current_model.decoder.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.SAVE_GRAD_FLOW_EVERY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training loop\n",
      "INFO:root:Step 1\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 2\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 3\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 4\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 5\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 6\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 7\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 8\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 9\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 10\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 11\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 12\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 13\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 14\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 15\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 16\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 17\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 18\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 19\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 20\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 21\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 22\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 23\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 24\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 25\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 26\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 27\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 28\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 29\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 30\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 31\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 32\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 33\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 34\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 35\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 36\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 37\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 38\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 39\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 40\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 41\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 42\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 43\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 44\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 45\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 46\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 47\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 48\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 49\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 50\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 51\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 52\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 53\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 54\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 55\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 56\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 57\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 58\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 59\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 60\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 61\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 62\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 63\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 64\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 65\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 66\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 67\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 68\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 69\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 70\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 71\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 72\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 73\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 74\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 75\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 76\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 77\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 78\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 79\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 80\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 81\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 82\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 83\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 84\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 85\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 86\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 87\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 88\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 89\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 90\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 91\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 92\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 93\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 94\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 95\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 96\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 97\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 98\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 99\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 100\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 101\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 102\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 103\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 104\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 105\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 106\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 107\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 108\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 109\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 110\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 111\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 112\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 113\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 114\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 115\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 116\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 117\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 118\n",
      "INFO:root:Sampling: Collecting new data\n",
      "INFO:root:Step 119\n",
      "INFO:root:Sampling: Collecting new data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0506bb46fc62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_checkpoint_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\rl-adventure\\rainbow_dialogues\\modules\\QLearning.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_steps, save_checkpoint_steps)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_update_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\rl-adventure\\rainbow_dialogues\\modules\\QLearning.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, batch, normalization)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mcurrent_net_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_net_attns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mtarget_net_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net_attns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;31m# pass through generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabian\\.virtualenvs\\rl-adventure-xhdax2fk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\rl-adventure\\rainbow_dialogues\\modules\\DQN.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, tgt, lengths, bptt)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mtgt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mtgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# exclude last target from inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0menc_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_bank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbptt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_bank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabian\\.virtualenvs\\rl-adventure-xhdax2fk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabian\\.virtualenvs\\rl-adventure-xhdax2fk\\lib\\site-packages\\onmt\\encoders\\rnn_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, lengths)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlengths\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_pack_padded_seq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m# Lengths data is wrapped inside a Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mlengths_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mpacked_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = trainer.train(train_steps=1000000, save_checkpoint_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for para in list(model.current_model.parameters()):\n",
    "#    printa(para.grad.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Replay Memory\n",
    "#for i, x in enumerate(model.replay_memory._storage[2000:]):\n",
    "#    if ' '.join([tgt_vocab.itos[token] for token in x[1].squeeze().tolist()]) != '<s> </s>':\n",
    "#        print(i+2000, ' '.join([src_vocab.itos[token] for token in x[0].squeeze().tolist()]) + '  ||  ' + ' '.join([tgt_vocab.itos[token] for token in x[1].squeeze().tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up rewards in replay memory\n",
    "#for i in range(0,len(model.replay_memory),2000):\n",
    "#    sum_ = sum([y[2] for y in model.replay_memory._storage[i:i+2000]])\n",
    "#    print(i, i+2000, sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at priorities in prioritzed replay memory\n",
    "#for i in range(0,len(model.replay_memory),2000):\n",
    "#    print(i, i+2000, sum([model.replay_memory._it_sum[y] for y in range(i, i+2000)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
